{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Q3**"
      ],
      "metadata": {
        "id": "d-VpZDUveYe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim import corpora, models\n",
        "\n",
        "# Set up NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load text from CSV\n",
        "def load_text_from_csv(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Skip header row\n",
        "        text = ' '.join(row[0] for row in reader)\n",
        "    return text\n",
        "\n",
        "# Preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and non-alphabetic characters, and convert to lowercase\n",
        "    words = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "    return words\n",
        "\n",
        "# Perform keyword extraction\n",
        "def extract_keywords(text, num_keywords=5):\n",
        "    freq_dist = nltk.FreqDist(text)\n",
        "    keywords = [word for word, _ in freq_dist.most_common(num_keywords)]\n",
        "    return keywords\n",
        "\n",
        "# Perform topic modeling\n",
        "def perform_topic_modeling(text, num_topics=5):\n",
        "    # Create dictionary and corpus\n",
        "    dictionary = corpora.Dictionary([text])\n",
        "    corpus = [dictionary.doc2bow(text)]\n",
        "\n",
        "    # Perform LDA topic modeling\n",
        "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
        "\n",
        "    topics = []\n",
        "    for topic in lda_model.print_topics(num_topics=num_topics):\n",
        "        topics.append(topic[1])\n",
        "\n",
        "    return topics\n",
        "\n",
        "# Main program\n",
        "if __name__ == '__main__':\n",
        "    # Load text from CSV\n",
        "    csv_file = 'output.csv'\n",
        "    text = load_text_from_csv(csv_file)\n",
        "\n",
        "    # Preprocess text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # Perform keyword extraction\n",
        "    keywords = extract_keywords(preprocessed_text, num_keywords=5)\n",
        "    print('Keywords:', keywords)\n",
        "\n",
        "    # Perform topic modeling\n",
        "    topics = perform_topic_modeling(preprocessed_text, num_topics=5)\n",
        "    print('Topics:')\n",
        "    for i, topic in enumerate(topics):\n",
        "        print(f'Topic {i+1}: {topic}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ1B1wy-m5Zv",
        "outputId": "a5d908e3-39c6-404e-fae1-f50c0372f4fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keywords: ['text', 'boring', 'file', 'simple', 'pdf']\n",
            "Topics:\n",
            "Topic 1: 0.515*\"text\" + 0.054*\"boring\" + 0.041*\"file\" + 0.028*\"pdf\" + 0.028*\"continued\" + 0.028*\"simple\" + 0.028*\"page\" + 0.015*\"use\" + 0.015*\"even\" + 0.015*\"virtual\"\n",
            "Topic 2: 0.041*\"text\" + 0.040*\"file\" + 0.040*\"boring\" + 0.040*\"page\" + 0.040*\"continued\" + 0.040*\"simple\" + 0.040*\"pdf\" + 0.040*\"watching\" + 0.040*\"mechanics\" + 0.040*\"demonstration\"\n",
            "Topic 3: 0.040*\"text\" + 0.040*\"file\" + 0.040*\"boring\" + 0.040*\"simple\" + 0.040*\"page\" + 0.040*\"pdf\" + 0.040*\"continued\" + 0.040*\"tutorials\" + 0.040*\"oh\" + 0.040*\"watching\"\n",
            "Topic 4: 0.040*\"text\" + 0.040*\"boring\" + 0.040*\"file\" + 0.040*\"page\" + 0.040*\"continued\" + 0.040*\"pdf\" + 0.040*\"simple\" + 0.040*\"little\" + 0.040*\"tutorials\" + 0.040*\"yet\"\n",
            "Topic 5: 0.040*\"text\" + 0.040*\"file\" + 0.040*\"boring\" + 0.040*\"page\" + 0.040*\"simple\" + 0.040*\"pdf\" + 0.040*\"continued\" + 0.040*\"watching\" + 0.040*\"tutorials\" + 0.040*\"yet\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQXBwdPkoL5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pDx8nZh0n7j5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}