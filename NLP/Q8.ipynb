{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Q8**"
      ],
      "metadata": {
        "id": "d-VpZDUveYe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Dataset\n",
        "dataset = {\n",
        "    'What is the capital of France?': 'The capital of France is Paris.',\n",
        "    'Who painted the Mona Lisa?': 'The Mona Lisa was painted by Leonardo da Vinci.',\n",
        "    # Add more questions and answers to the dataset\n",
        "}\n",
        "\n",
        "# Set up NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and non-alphabetic characters, and lemmatize the words\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "    return words\n",
        "\n",
        "# Calculate similarity between two texts using Jaccard similarity\n",
        "def calculate_similarity(text1, text2):\n",
        "    set1 = set(text1)\n",
        "    set2 = set(text2)\n",
        "    intersection = set1.intersection(set2)\n",
        "    union = set1.union(set2)\n",
        "    similarity = len(intersection) / len(union)\n",
        "    return similarity\n",
        "\n",
        "# Find the best matching question from the dataset\n",
        "def find_matching_question(user_input):\n",
        "    preprocessed_input = preprocess_text(user_input)\n",
        "    best_question = None\n",
        "    best_similarity = 0\n",
        "\n",
        "    for question in dataset.keys():\n",
        "        preprocessed_question = preprocess_text(question)\n",
        "        similarity = calculate_similarity(preprocessed_input, preprocessed_question)\n",
        "\n",
        "        if similarity > best_similarity:\n",
        "            best_similarity = similarity\n",
        "            best_question = question\n",
        "\n",
        "    return best_question\n",
        "\n",
        "# Build knowledge bot\n",
        "def build_knowledge_bot():\n",
        "    print(\"Knowledge Bot: Hello! I am a knowledge bot. Ask me anything!\")\n",
        "    while True:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Find the best matching question from the dataset\n",
        "        matching_question = find_matching_question(user_input)\n",
        "\n",
        "        if matching_question:\n",
        "            answer = dataset[matching_question]\n",
        "            print(\"Knowledge Bot:\", answer)\n",
        "        else:\n",
        "            print(\"Knowledge Bot: I'm sorry, but I don't have the answer to that question.\")\n",
        "\n",
        "# Main program\n",
        "if __name__ == '__main__':\n",
        "    # Build knowledge bot\n",
        "    build_knowledge_bot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHUzUlz2pt9_",
        "outputId": "0689bb38-b6bb-457d-ca92-42a88d1d52d6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Knowledge Bot: Hello! I am a knowledge bot. Ask me anything!\n",
            "User: France capital\n",
            "Knowledge Bot: The capital of France is Paris.\n",
            "User: Mona\n",
            "Knowledge Bot: The Mona Lisa was painted by Leonardo da Vinci.\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7wieiQdBqBqG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}