{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Q1**"
      ],
      "metadata": {
        "id": "d-VpZDUveYe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora, models\n",
        "from googleapiclient.discovery import build\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to preprocess comments\n",
        "def preprocess_comments(comments):\n",
        "    # Tokenize comments\n",
        "    tokenized_comments = [word_tokenize(comment.lower()) for comment in comments]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_comments = [[word for word in comment if word not in stop_words] for comment in tokenized_comments]\n",
        "\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_comments = [[lemmatizer.lemmatize(word) for word in comment] for comment in filtered_comments]\n",
        "\n",
        "    return lemmatized_comments\n",
        "\n",
        "# Function to perform topic modeling\n",
        "def perform_topic_modeling(comments):\n",
        "    dictionary = corpora.Dictionary(comments)\n",
        "    corpus = [dictionary.doc2bow(comment) for comment in comments]\n",
        "    lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
        "    topics = lda_model.print_topics(num_words=5)\n",
        "\n",
        "    return topics\n",
        "\n",
        "# Function to authenticate and retrieve comments using YouTube Data API\n",
        "def get_video_comments(api_key, video_id):\n",
        "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "    comments = []\n",
        "    nextPageToken = None\n",
        "\n",
        "    while True:\n",
        "        response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            textFormat='plainText',\n",
        "            pageToken=nextPageToken,\n",
        "            maxResults=100\n",
        "        ).execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "\n",
        "        if 'nextPageToken' in response:\n",
        "            nextPageToken = response['nextPageToken']\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return comments\n",
        "\n",
        "# Get YouTube video URL and API key from user\n",
        "video_url = input(\"Enter the YouTube video URL: \")\n",
        "api_key = input(\"Enter your YouTube Data API key: \")\n",
        "\n",
        "# Extract video ID from the URL\n",
        "video_id = video_url.split('v=')[-1]\n",
        "\n",
        "# Retrieve comments using YouTube Data API\n",
        "comments = get_video_comments(api_key, video_id)\n",
        "\n",
        "# Store comments in a CSV file\n",
        "df = pd.DataFrame(comments, columns=['Comment'])\n",
        "df.to_csv('youtube_comments.csv', index=False)\n",
        "\n",
        "# Preprocess comments\n",
        "preprocessed_comments = preprocess_comments(comments)\n",
        "\n",
        "# Perform topic modeling\n",
        "topics = perform_topic_modeling(preprocessed_comments)\n",
        "\n",
        "# Print the identified topics\n",
        "for topic in topics:\n",
        "    print(topic)"
      ],
      "metadata": {
        "id": "mS6NmkV9lmto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XVnGJVyYlC6U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}